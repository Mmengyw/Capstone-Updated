{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mmengyw/Capstone-Updated/blob/main/SAR%2BCamera_Fusion/SAR_and_Camera_PointCloud_Apr18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjBDj97y9k18"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vwz1UzMu9mF-",
        "outputId": "d91fd756-e054-4ae2-be5d-43b59b90e725"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "# Tensorflow\n",
        "import tensorflow.compat.v1 as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# I/O libraries\n",
        "import os\n",
        "from io import BytesIO\n",
        "import tarfile\n",
        "import tempfile\n",
        "from six.moves import urllib\n",
        "\n",
        "# Helper libraries\n",
        "import matplotlib\n",
        "import torch\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2 as cv\n",
        "from tqdm import tqdm\n",
        "import IPython\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Comment this out if you want to see Deprecation warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "#test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwkf2tnYQKAT",
        "outputId": "9b301478-df05-4cc3-e8bd-71ac21de00f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[K     |████████████████████████████████| 431 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ],
      "source": [
        "pip install timm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jKEwPDK-Edp"
      },
      "source": [
        "# Functions for Image Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24Xfj_GZ-FWL"
      },
      "outputs": [],
      "source": [
        "class DeepLabModel(object):\n",
        "    \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
        "\n",
        "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
        "\n",
        "    def __init__(self, tarball_path):\n",
        "        \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
        "        self.graph = tf.Graph()\n",
        "        graph_def = None\n",
        "\n",
        "        # Extract frozen graph from tar archive.\n",
        "        tar_file = tarfile.open(tarball_path)\n",
        "        for tar_info in tar_file.getmembers():\n",
        "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
        "                file_handle = tar_file.extractfile(tar_info)\n",
        "                graph_def = tf.GraphDef.FromString(file_handle.read())\n",
        "                break\n",
        "        tar_file.close()\n",
        "\n",
        "        if graph_def is None:\n",
        "            raise RuntimeError('Cannot find inference graph in tar archive.')\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            tf.import_graph_def(graph_def, name='')\n",
        "        self.sess = tf.Session(graph=self.graph)\n",
        "\n",
        "    def run(self, image, INPUT_TENSOR_NAME = 'ImageTensor:0', OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'):\n",
        "        \"\"\"Runs inference on a single image.\n",
        "\n",
        "        Args:\n",
        "            image: A PIL.Image object, raw input image.\n",
        "            INPUT_TENSOR_NAME: The name of input tensor, default to ImageTensor.\n",
        "            OUTPUT_TENSOR_NAME: The name of output tensor, default to SemanticPredictions.\n",
        "\n",
        "        Returns:\n",
        "            resized_image: RGB image resized from original input image.\n",
        "            seg_map: Segmentation map of `resized_image`.\n",
        "        \"\"\"\n",
        "        width, height = image.size\n",
        "        target_size = (2049,1025)  # size of Cityscapes images\n",
        "        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
        "        batch_seg_map = self.sess.run(\n",
        "            OUTPUT_TENSOR_NAME,\n",
        "            feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
        "        seg_map = batch_seg_map[0]  # expected batch size = 1\n",
        "        if len(seg_map.shape) == 2:\n",
        "            seg_map = np.expand_dims(seg_map,-1)  # need an extra dimension for cv.resize\n",
        "        seg_map = cv.resize(seg_map, (width,height), interpolation=cv.INTER_NEAREST)\n",
        "        return seg_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCIUhvfV-MiI"
      },
      "outputs": [],
      "source": [
        "def create_label_colormap():\n",
        "    \"\"\"Creates a label colormap used in Cityscapes segmentation benchmark.\n",
        "\n",
        "    Returns:\n",
        "        A Colormap for visualizing segmentation results.\n",
        "    \"\"\"\n",
        "    colormap = np.array([\n",
        "        [128,  64, 128],\n",
        "        [244,  35, 232],\n",
        "        [ 70,  70,  70],\n",
        "        [102, 102, 156],\n",
        "        [190, 153, 153],\n",
        "        [153, 153, 153],\n",
        "        [250, 170,  30],\n",
        "        [220, 220,   0],\n",
        "        [107, 142,  35],\n",
        "        [152, 251, 152],\n",
        "        [ 70, 130, 180],\n",
        "        [220,  20,  60],\n",
        "        [255,   0,   0],\n",
        "        [  0,   0, 142],\n",
        "        [  0,   0,  70],\n",
        "        [  0,  60, 100],\n",
        "        [  0,  80, 100],\n",
        "        [  0,   0, 230],\n",
        "        [119,  11,  32],\n",
        "        [  0,   0,   0]], dtype=np.uint8)\n",
        "    return colormap\n",
        "\n",
        "\n",
        "def label_to_color_image(label):\n",
        "    \"\"\"Adds color defined by the dataset colormap to the label.\n",
        "\n",
        "    Args:\n",
        "        label: A 2D array with integer type, storing the segmentation label.\n",
        "\n",
        "    Returns:\n",
        "        result: A 2D array with floating type. The element of the array\n",
        "            is the color indexed by the corresponding element in the input label\n",
        "            to the PASCAL color map.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If label is not of rank 2 or its value is larger than color\n",
        "            map maximum entry.\n",
        "    \"\"\"\n",
        "    if label.ndim != 2:\n",
        "        raise ValueError('Expect 2-D input label')\n",
        "\n",
        "    colormap = create_label_colormap()\n",
        "\n",
        "    if np.max(label) >= len(colormap):\n",
        "        raise ValueError('label value too large.')\n",
        "\n",
        "    return colormap[label]\n",
        "\n",
        "\n",
        "def vis_segmentation(image, seg_map):\n",
        "    \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n",
        "\n",
        "    plt.subplot(grid_spec[0])\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title('input image')\n",
        "\n",
        "    plt.subplot(grid_spec[1])\n",
        "    seg_image = label_to_color_image(seg_map).astype(np.uint8)\n",
        "    plt.imshow(seg_image)\n",
        "    plt.axis('off')\n",
        "    plt.title('segmentation map')\n",
        "\n",
        "    plt.subplot(grid_spec[2])\n",
        "    plt.imshow(image)\n",
        "    plt.imshow(seg_image, alpha=0.7)\n",
        "    plt.axis('off')\n",
        "    plt.title('segmentation overlay')\n",
        "\n",
        "    unique_labels = np.unique(seg_map)\n",
        "    ax = plt.subplot(grid_spec[3])\n",
        "    plt.imshow(FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n",
        "    ax.yaxis.tick_right()\n",
        "    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n",
        "    plt.xticks([], [])\n",
        "    ax.tick_params(width=0.0)\n",
        "    plt.grid('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "LABEL_NAMES = np.asarray([\n",
        "    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "    'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck',\n",
        "    'bus', 'train', 'motorcycle', 'bicycle', 'void'])\n",
        "\n",
        "COLOR_MAP = np.array([\n",
        "    [128,  64, 128],\n",
        "    [244,  35, 232],\n",
        "    [ 70,  70,  70],\n",
        "    [102, 102, 156],\n",
        "    [190, 153, 153],\n",
        "    [153, 153, 153],\n",
        "    [250, 170,  30],\n",
        "    [220, 220,   0],\n",
        "    [107, 142,  35],\n",
        "    [152, 251, 152],\n",
        "    [ 70, 130, 180],\n",
        "    [220,  20,  60],\n",
        "    [255,   0,   0],\n",
        "    [  0,   0, 142],\n",
        "    [  0,   0,  70],\n",
        "    [  0,  60, 100],\n",
        "    [  0,  80, 100],\n",
        "    [  0,   0, 230],\n",
        "    [119,  11,  32],\n",
        "    [  0,   0,   0]], dtype=np.uint8)\n",
        "\n",
        "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
        "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxKiAGG_-QYw",
        "outputId": "3badb03f-c46d-4319-8c6b-ccb3d41892b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading model, this might take a while...\n",
            "download completed! loading DeepLab model...\n",
            "model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = 'mobilenetv2_coco_cityscapes_trainfine'\n",
        "#MODEL_NAME = 'xception65_cityscapes_trainfine'\n",
        "\n",
        "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
        "_MODEL_URLS = {\n",
        "    'mobilenetv2_coco_cityscapes_trainfine':\n",
        "        'deeplabv3_mnv2_cityscapes_train_2018_02_05.tar.gz',\n",
        "    'xception65_cityscapes_trainfine':\n",
        "        'deeplabv3_cityscapes_train_2018_02_06.tar.gz',\n",
        "}\n",
        "_TARBALL_NAME = 'deeplab_model.tar.gz'\n",
        "\n",
        "model_dir = tempfile.mkdtemp()\n",
        "tf.gfile.MakeDirs(model_dir)\n",
        "\n",
        "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
        "print('downloading model, this might take a while...')\n",
        "urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME], download_path)\n",
        "print('download completed! loading DeepLab model...')\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n",
        "print('model loaded successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVnUbIVY96HU"
      },
      "source": [
        "# Initialize Midas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "9e79f101c1b7442385207bc9fe00ddc2",
            "a6967a9a72f74b5682b3e3cdccdde067",
            "c1104dc2c35e4457a7909ec8561a9aca",
            "68c7b7f676f44342baecebdfd269fd76",
            "4e4260cf5e474aeba86cfcffeeaa434c",
            "7b51a34012af49e18f6c82c2c9dc9d6b",
            "f9251aaa927f4cc996241ae1f75cd831",
            "369c511214ca49fdaa371b31f9b460ae",
            "aab355a9fbb746688d3eae5a2eb1559d",
            "e84fffe5024c4657807ddd9d3400ee28",
            "f63a2c3e4f66442999a9634d79829438"
          ]
        },
        "id": "uW1E_emiQatf",
        "outputId": "e1c3174a-6496-4c2d-ff79-ecc0d7d1c0f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/intel-isl/MiDaS/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "Downloading: \"https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt\" to /root/.cache/torch/hub/checkpoints/dpt_large-midas-2f21e586.pt\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e79f101c1b7442385207bc9fe00ddc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/1.28G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n",
        "#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n",
        "#model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n",
        "\n",
        "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywJyJpTzQtDR",
        "outputId": "2587e20d-d64a-43a1-8b33-1ab739cbf65d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DPTDepthModel(\n",
              "  (pretrained): Module(\n",
              "    (model): VisionTransformer(\n",
              "      (patch_embed): PatchEmbed(\n",
              "        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
              "        (norm): Identity()\n",
              "      )\n",
              "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "      (blocks): Sequential(\n",
              "        (0): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (12): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (13): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (14): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (15): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (16): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (17): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (18): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (19): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (20): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (21): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (22): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (23): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (pre_logits): Identity()\n",
              "      (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
              "    )\n",
              "    (act_postprocess1): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU()\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
              "    )\n",
              "    (act_postprocess2): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU()\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (act_postprocess3): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU()\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (act_postprocess4): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU()\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (scratch): Module(\n",
              "    (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (refinenet1): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet2): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet3): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet4): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (output_conv): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): Interpolate()\n",
              "      (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (5): ReLU(inplace=True)\n",
              "      (6): Identity()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "midas.to(device)\n",
        "midas.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4Da96O0Q06Z",
        "outputId": "0f77ab72-71d8-40b0-d43c-9234b918148e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        }
      ],
      "source": [
        "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "\n",
        "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
        "    transform = midas_transforms.dpt_transform\n",
        "else:\n",
        "    transform = midas_transforms.small_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN0kU6NJ1Ye5",
        "outputId": "c5bbf1bb-fa56-4d0e-e38a-1804e27c0749"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ],
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdsigyfAhITX",
        "outputId": "0ad0cb67-bc15-4937-ea94-cf0c81925109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Capstone-Updated'...\n",
            "remote: Enumerating objects: 203, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 203 (delta 96), reused 35 (delta 15), pack-reused 18\u001b[K\n",
            "Receiving objects: 100% (203/203), 217.22 MiB | 9.79 MiB/s, done.\n",
            "Resolving deltas: 100% (97/97), done.\n",
            "Capstone-Updated  sample_data\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Mmengyw/Capstone-Updated.git\n",
        "!ls\n",
        "os.chdir(\"Capstone-Updated/Videos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kja3q7PyhB4H"
      },
      "source": [
        "# Conversion to meters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoPzjnl5hCCS"
      },
      "outputs": [],
      "source": [
        "nb_photo=34\n",
        "\n",
        "equiv=[[0.001,51],    #for extrapolation\n",
        "      [0.1,45],     #premier plan\n",
        "      [0.9,42.3],\n",
        "      [1.8,37.4],\n",
        "      [2.7,28.7],\n",
        "      [3.6,24.443365],\n",
        "      [4.5,22.058018],\n",
        "      [5.4,15.317413],\n",
        "      [6.3,14.677493],\n",
        "      [7.2,10.969739],\n",
        "      [8.1,10.883035],\n",
        "      [9,9.883035],\n",
        "      [9.9,8.058806],\n",
        "      [10.8,7.5158963],\n",
        "      [11.7,7.098169],\n",
        "      [12.6,6.111024],\n",
        "      [13.5,5.6323136],\n",
        "      [14.4,5.2216917],\n",
        "      [15.3,5],\n",
        "      [16.2,4.9529667],\n",
        "      [17.1,4.8],\n",
        "      [18,4.7],\n",
        "      [18.9,4.6],\n",
        "      [19.8,4.5],\n",
        "      [20.7,4.4],\n",
        "      [21.6,4.3],\n",
        "      [22.5,4.2],\n",
        "      [23.4,4.1],\n",
        "      [24.3,4],\n",
        "      [25.2,3.9],\n",
        "      [26.1,3.8],\n",
        "      [27,3.7],\n",
        "      [27.9,3.6],\n",
        "      [28.8,3.5],\n",
        "      [29.7,3.2],\n",
        "      [30.6,3],\n",
        "      [60,0.0],\n",
        "       \n",
        "      [120,-6],    #horizon\n",
        "       \n",
        "      [40,1.98],\n",
        "      [50,1.33]\n",
        "       \n",
        "       ]   #for extrapolation\n",
        "\n",
        "#=========================================================================================\n",
        "\n",
        "equiv2=[[1,41.05157], #1yard  0302\n",
        "        [1,42.18351],\n",
        "        [1,31.304607],\n",
        "        [1,25.090006],\n",
        "        [1,23.275448], #5yard 0306\n",
        "        [1,19.171278],\n",
        "        [1,17.472866],\n",
        "        [1,16.775742],\n",
        "        [1,15.820402],\n",
        "        [1,15.538459], #10yard  0311\n",
        "        [1,14.466544],\n",
        "        [1,12.707126],\n",
        "        [1,10.957558],\n",
        "      \n",
        "\n",
        "        [1,6.023936],#'''inacurrate'''\n",
        "        [1,9.797453],  #15yard 0316\n",
        "        [1,7.2150397],\n",
        "        [1,6.3944836],\n",
        "        [1,8.514687],\n",
        "        [1,7.735209],\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt4BAs1zhPp3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import Rbf, InterpolatedUnivariateSpline\n",
        "equiv=np.asarray(equiv)\n",
        "X = equiv[:,1]    #midas output\n",
        "Y=equiv[:,0]     #meters\n",
        "new_length = 25\n",
        "new_x = np.linspace(X.min(), X.max(), new_length)\n",
        "conv=sp.interpolate.interp1d(X, Y, kind='linear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2Fd1wKnhUqf"
      },
      "source": [
        "#Increased contrast\n",
        "Just to get increased contrast, not values in meters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpDq3tgxhQV4"
      },
      "outputs": [],
      "source": [
        "coef_expand=[[-5,-1],\n",
        "        [1,10],\n",
        "        [5,25],\n",
        "        [10,35],\n",
        "        [15,42],\n",
        "        [35,43],\n",
        "        [45,50],\n",
        "        [51,51]\n",
        "        ]\n",
        "coef_expand=np.asarray(coef_expand)\n",
        "X = coef_expand[:,0]    #midas output\n",
        "Y=coef_expand[:,1]      #meters\n",
        "expand=sp.interpolate.interp1d(X, Y, kind='cubic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99un1SR_xll"
      },
      "source": [
        "# Segmented Point Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-QtEEGNGcuH"
      },
      "outputs": [],
      "source": [
        "from math import sin,cos,atan2 \n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "def depth2pcd_segm(depth,seg_map):\n",
        "    # print(depth)\n",
        "    # print(seg_map)\n",
        "    width = depth.shape[1]\n",
        "    height = depth.shape[0]\n",
        "    # print(width)\n",
        "    # print(height)\n",
        "    fx= 926.9796142578125\n",
        "    fy= 924.431884765625\n",
        "    cx= 790.234375\n",
        "    cy= 617.5499267578125\n",
        "    points = []\n",
        "    objects_needed = {'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "    'traffic sign', 'vegetation', 'terrain', 'person', 'rider', 'car', 'truck',\n",
        "    'bus', 'train', 'motorcycle', 'bicycle'}\n",
        "    objects_car = {'car'}\n",
        "    objects_wanted = {'car', 'trees','sidewalk'}\n",
        "\n",
        "\n",
        "    for v in range(0, width, 5):\n",
        "        # for u in range(0, height, 2):\n",
        "        for u in range(0, 1000, 5):\n",
        "            R = depth[u][v]\n",
        "            color = seg_map[u][v]\n",
        "            # print(R)\n",
        "            # print(color)\n",
        "            if R == 0:\n",
        "                continue\n",
        "            \n",
        "            X_cam = (v - cx)\n",
        "            Y_cam = -(u - cy)\n",
        "\n",
        "            theta_x = atan2(X_cam,fx)\n",
        "            theta_y = atan2(Y_cam,fy)\n",
        "\n",
        "            X = R*cos(theta_y)*sin(theta_x)\n",
        "            Y = R*cos(theta_x)*sin(theta_y)\n",
        "            Z = R*cos(theta_x)*cos(theta_y)\n",
        "\n",
        "            if LABEL_NAMES[color] in objects_wanted:\n",
        "              # points.append([X, Y, Z])\n",
        "              points.append([X, Y, Z,color])\n",
        "            # points.append([X, Y, Z,color])\n",
        "            \n",
        "    return np.array(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "refmmrWb_2eH"
      },
      "outputs": [],
      "source": [
        "def prediction_stream(image, seg_map, seg_data, frame, index):\n",
        "    \"\"\"Visualizes segmentation overlay view and stream it with IPython display.\"\"\"\n",
        "    for i in range(len(seg_map)):\n",
        "        for j in range(len(seg_map[i])):\n",
        "                seg_data[i][j] = LABEL_NAMES[seg_map[i][j]]\n",
        "                \n",
        "\n",
        "    \n",
        "    img = frame\n",
        "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "\n",
        "    input_batch = transform(img).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      prediction = midas(input_batch)\n",
        "\n",
        "      prediction = torch.nn.functional.interpolate(\n",
        "        prediction.unsqueeze(1),\n",
        "        size=img.shape[:2],\n",
        "        mode=\"bicubic\",\n",
        "        align_corners=False,\n",
        "      ).squeeze()\n",
        "\n",
        "    output = prediction.cpu().numpy()\n",
        "    output = (output > 0) * output\n",
        "    distanceGuess = 2\n",
        "    alpha = output[output.shape[0]-10, int(output.shape[1]*3/4)]*distanceGuess\n",
        "    # output=expand(50*output/np.max(output)) # for better visualization\n",
        "    # output=conv(50*output/np.max(output)) \n",
        "    output = alpha/(output+.001)\n",
        "    # print(output) # to get values in meters\n",
        "\n",
        "    # plt.imshow(output, cmap='plasma')\n",
        "\n",
        "    pc_3d = depth2pcd_segm(output,seg_map)\n",
        "    # if len(pc_3d) == 0:\n",
        "    #   fig = plt.figure(figsize=(12,6))\n",
        "    # else:\n",
        "    #   x = pc_3d[:, 0]\n",
        "    #   y = pc_3d[:, 1]\n",
        "    #   z = pc_3d[:, 2]\n",
        "    #   color = pc_3d[:, 3]\n",
        "    #   color_plot = np.zeros((len(color),3))\n",
        "    #   for i in range(len(color)):\n",
        "    #     color_plot[i] = COLOR_MAP[int(color[i])]\n",
        "\n",
        "    #   fig = plt.figure(figsize=(12,6))\n",
        "      # pc3dd = np.array([[x[i], z[i]] for i in range(len(x))])\n",
        "      # km = KMeans(n_clusters = 4)\n",
        "      # clusters= km.fit_predict(pc3dd)\n",
        "      # centroids = km.cluster_centers_\n",
        "      # points = np.empty((0,len(pc3dd[0])), float)\n",
        "      # # distances will be used to calculate outliers\n",
        "      # distances = np.empty((0,len(pc3dd[0])), float)\n",
        "      # # getting points and distances\n",
        "      # for i, center_elem in enumerate(centroids):\n",
        "      #     # cdist is used to calculate the distance between center and other points\n",
        "      #     distances = np.append(distances, cdist([center_elem],pc3dd[clusters == i], 'euclidean')) \n",
        "      #     points = np.append(points, pc3dd[clusters == i], axis=0)\n",
        "      # percentile = 90\n",
        "      # # getting outliers whose distances are greater than some percentile\n",
        "      # outliers = points[np.where(distances > np.percentile(distances, percentile))]\n",
        "      # #plotting outliers\n",
        "      # pc3dlast = C = np.array(list(filter(lambda x: x not in outliers, pc3dd)))\n",
        "      # plt.scatter(pc3dlast[:,0],pc3dlast[:,1],s=0.1)\n",
        "    #   plt.scatter(x,z,c=color_plot/255,s=0.1)\n",
        "    # plt.xlabel('Z')\n",
        "    # plt.ylabel('X')\n",
        "    # plt.xlim(-30, 30)\n",
        "    # plt.ylim(0, 30) \n",
        "\n",
        "    # plt.savefig('saved_figure.jpg')\n",
        "    # im = cv.imread('saved_figure.jpg')\n",
        "    # frames.append(im)\n",
        "    # plt.close()\n",
        "    # plt.imshow(expand(output), cmap='plasma')\n",
        "    # A = np.stack([seg_data,output])\n",
        "    # A = np.stack([seg_map,output])\n",
        "\n",
        "    # # Show visualization in a streaming fashion.\n",
        "    # f = BytesIO()\n",
        "    # plt.savefig(f, format='jpeg')\n",
        "    # IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
        "    # f.close()\n",
        "    # plt.close()\n",
        "    return pc_3d\n",
        "def prediction_video(frame, index):\n",
        "    \"\"\"Inferences DeepLab model on a video file and stream the visualization.\"\"\"\n",
        "    original_im = Image.fromarray(frame[..., ::-1])\n",
        "    seg_map = MODEL.run(original_im)\n",
        "    seg_data = np.full((len(seg_map),len(seg_map[0])),'nullvoidnada')\n",
        "    filled_seg_data = prediction_stream(original_im, seg_map, seg_data, frame, index)\n",
        "    # print(filled_seg_data)\n",
        "    return filled_seg_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV-Kk28ldELA",
        "outputId": "12d80d8a-510c-4d98-f1b4-9dbf4894d8c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ],
      "source": [
        "# Get Map of Time frames of SAR to Camera, by finding nearest frames\n",
        "\n",
        "import pickle\n",
        "# os.chdir(\"Capstone-Updated/SAR+Camera_Fusion\")\n",
        "\n",
        "with open('camera_times.pickle', 'rb') as file:\n",
        "    camera_times = pickle.load(file)\n",
        "\n",
        "with open('sar_tracklog.pickle', 'rb') as file:\n",
        "    SAR_tracklog = pickle.load(file)\n",
        "\n",
        "SAR_tracklog = np.array(SAR_tracklog)\n",
        "SAR_times = SAR_tracklog[:,0]\n",
        "\n",
        "timestamp_map = np.zeros(len(SAR_times))\n",
        "\n",
        "j=0\n",
        "for i in range(len(SAR_times)):\n",
        "  while camera_times[j]<SAR_times[i]:\n",
        "    j+=1\n",
        "  timestamp_map[i] = j\n",
        "# print(timestamp_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7a_YXpaEno-",
        "outputId": "9926b4b9-6fe5-4a56-9928-6a669f7301c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "499\n"
          ]
        }
      ],
      "source": [
        "frames = []\n",
        "pc3darray = []\n",
        "# SAMPLE_VIDEO = 'camera30fps.mp4'\n",
        "SAMPLE_VIDEO = 'camera.mp4'\n",
        "print('running deeplab on the sample video...')\n",
        "\n",
        "video = cv.VideoCapture(SAMPLE_VIDEO)\n",
        "# num_frames = 598  # uncomment to use the full sample video\n",
        "num_frames = 500\n",
        "\n",
        "try:\n",
        "    for i in range(num_frames):\n",
        "        _, frame = video.read()\n",
        "        if not _: break\n",
        "        print(i)\n",
        "        filled_seg_DATA = prediction_video(frame, int(timestamp_map[i]))\n",
        "        pc3darray.append(filled_seg_DATA)\n",
        "        IPython.display.clear_output(wait=True)\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n",
        "# print(filled_seg_DATA)\n",
        "\n",
        "\n",
        "# height, width, layers = frames[0].shape\n",
        "# size = (width,height)\n",
        "# fourcc = cv.VideoWriter_fourcc(*'MJPG')\n",
        "# out = cv.VideoWriter('new_3D_pointcloud_wanted.avi', fourcc, 30.0, size)\n",
        " \n",
        "# for i in range(len(frames)):\n",
        "#     out.write(frames[i])\n",
        "# out.release()\n",
        "\n",
        "# pc3darray = np.array(pc3darray)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5OVRWSXYUo4"
      },
      "source": [
        "# Convert Camera Frame to SAR frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe_Qnn-DYafi"
      },
      "outputs": [],
      "source": [
        "#Transform a point [X,Y,Z] from the camera frame to the car frame (SAR)\n",
        "def Cam_ref_2_Car_ref(Pos_obj_cam):\n",
        "    #camera extrinsic (quaternion, translation)\n",
        "    R=[[ 0.99994752,  0.00325207,  0.00971481],\n",
        "    [-0.0030831 ,  0.99984459, -0.01735761],\n",
        "    [-0.00976975,  0.01732675,  0.99980215]]\n",
        "\n",
        "    T=[-0.41649988293647766, 0.09146018326282501, 0.011436160653829575]\n",
        "\n",
        "    Pos_obj_car = R@Pos_obj_cam[:3] + T\n",
        "    Pos_obj_car = np.append(Pos_obj_car,[Pos_obj_cam[-1]])\n",
        "    return Pos_obj_car"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tucOmzqbSK-",
        "outputId": "56f1b45e-f2e4-4d97-f075-9bd87889690c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ],
      "source": [
        "pc3darray_SAR_frame = []\n",
        "\n",
        "for i in range(len(pc3darray)):\n",
        "  pointcloud = []\n",
        "  for j in range(len(pc3darray[i])):\n",
        "    pointcloud.append(Cam_ref_2_Car_ref(pc3darray[i][j]))\n",
        "  pc3darray_SAR_frame.append(pointcloud)\n",
        "\n",
        "# print(Cam_ref_2_Car_ref(pc3darray[10]))\n",
        "\n",
        "pc3darray_SAR_frame = np.array(pc3darray_SAR_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFxbZhGc1Anl",
        "outputId": "4ac9abb6-6cd6-47cc-aaa9-90ad07720c07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-10.05950406  -1.65349278  11.51368624   1.        ]\n",
            "1.0\n",
            "[244  35 232]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "coord = np.array(pc3darray_SAR_frame[39])\n",
        "print(coord[1])\n",
        "plt.scatter(coord[:,0],coord[:,2],s = 0.2)\n",
        "print(coord[1,3])\n",
        "print(COLOR_MAP[int(coord[1,3])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7q2x5oGBQdo",
        "outputId": "e1998e49-ded8-4005-dc1e-1ea3951555bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1.0, 13.0}\n"
          ]
        }
      ],
      "source": [
        "col = set(coord[:,3])\n",
        "print(col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKc6MK8Hy_YJ"
      },
      "source": [
        "# SAR Video Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ32WSdJzDmb"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "from io import BytesIO\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# SHRAVANs DRIVE\n",
        "# !ls \"/content/drive/My Drive/Capstone/SAR_data\"\n",
        "\n",
        "RADAR_VIDEO = 'radar_sar.mp4'\n",
        "CAMERA_VIDEO = 'camera.mp4'\n",
        "\n",
        "# # OMs DRIVE\n",
        "# !ls \"/content/drive/My Drive/Capstone/SAR_data\"\n",
        "\n",
        "# RADAR_VIDEO = '/content/drive/My Drive/Capstone/SAR_data/radar_sar.mp4'\n",
        "# CAMERA_VIDEO = '/content/drive/My Drive/Capstone/SAR_data/camera.mp4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqhiiwnSHaB8"
      },
      "outputs": [],
      "source": [
        "def Convert_to_Meters(frame):\n",
        "  center_x = 625\n",
        "  center_y = 624\n",
        "  height, width = frame.shape\n",
        "\n",
        "  points = []\n",
        "\n",
        "  for v in range(0, width):\n",
        "        for u in range(0, height):\n",
        "          if frame[u][v]<200:\n",
        "            x = (v-center_x)*0.04\n",
        "            y = -(u-center_y)*0.04\n",
        "            points.append([x,y])\n",
        "  return np.array(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlWuU-x-zXSF"
      },
      "outputs": [],
      "source": [
        "def GetThreshold_Binary(frame):\n",
        "  gray_scale = 255 - cv.cvtColor(frame,cv.COLOR_RGB2GRAY)\n",
        "  threshold = 0.9*np.max(gray_scale)\n",
        "  _, thres = cv.threshold(gray_scale, threshold, 255,cv.THRESH_BINARY)\n",
        "  point_cloud = Convert_to_Meters(thres)\n",
        "  point_cloud = np.array(point_cloud)\n",
        "  \n",
        "  # fig = plt.figure(figsize=(12,6))\n",
        "  # plt.scatter(point_cloud[:,0],point_cloud[:,1],s=0.1)\n",
        "  # plt.xlabel('Z')\n",
        "  # plt.ylabel('X')\n",
        "  # plt.xlim(-25, 25)\n",
        "  # plt.ylim(0, 25) \n",
        "\n",
        "  # plt.savefig('saved_figure.jpg')\n",
        "  # im = cv.imread('saved_figure.jpg')\n",
        "  # frames.append(im)\n",
        "  # plt.close()\n",
        "\n",
        "  return thres, point_cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msOjRuVCHRjQ"
      },
      "outputs": [],
      "source": [
        "def GetThreshold_Adaptive(frame):\n",
        "  gray_scale = 255 - cv.cvtColor(frame,cv.COLOR_RGB2GRAY)\n",
        "  thres = cv.adaptiveThreshold(gray_scale,255,cv.ADAPTIVE_THRESH_MEAN_C,cv.THRESH_BINARY,11,10)\n",
        "  point_cloud = Convert_to_Meters(thres)\n",
        "  point_cloud = np.array(point_cloud)\n",
        "  \n",
        "  # fig = plt.figure(figsize=(12,6))\n",
        "  # plt.scatter(point_cloud[:,0],point_cloud[:,1],s=0.1)\n",
        "  # plt.xlabel('Z')\n",
        "  # plt.ylabel('X')\n",
        "  # plt.xlim(-25, 25)\n",
        "  # plt.ylim(0, 25) \n",
        "\n",
        "  # plt.savefig('saved_figure.jpg')\n",
        "  # im = cv.imread('saved_figure.jpg')\n",
        "  # frames.append(im)\n",
        "  # plt.close()\n",
        "\n",
        "  return thres, point_cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdGGz5yHHTOy"
      },
      "outputs": [],
      "source": [
        "def GetThreshold_Gaussian(frame):\n",
        "  gray_scale = 255 - cv.cvtColor(frame,cv.COLOR_RGB2GRAY)\n",
        "  thres = cv.adaptiveThreshold(gray_scale,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY,21,10)\n",
        "  point_cloud = Convert_to_Meters(thres)\n",
        "  point_cloud = np.array(point_cloud)\n",
        "  \n",
        "  # fig = plt.figure(figsize=(12,6))\n",
        "  # plt.scatter(point_cloud[:,0],point_cloud[:,1],s=0.1)\n",
        "  # plt.xlabel('Z')\n",
        "  # plt.ylabel('X')\n",
        "  # plt.xlim(-25, 25)\n",
        "  # plt.ylim(0, 25) \n",
        "\n",
        "  # plt.savefig('saved_figure.jpg')\n",
        "  # im = cv.imread('saved_figure.jpg')\n",
        "  # frames.append(im)\n",
        "  # plt.close()\n",
        "\n",
        "  return thres, point_cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o--rPYmC54_a"
      },
      "outputs": [],
      "source": [
        "def Plot_Camera_with_SAR(point_cloud_SAR, point_cloud_Camera):\n",
        "  \n",
        "  fig = plt.figure(figsize=(12,6))\n",
        "  plt.scatter(point_cloud_SAR[:,0],point_cloud_SAR[:,1],s=0.1, color = 'black')\n",
        "\n",
        "  if len(point_cloud_Camera)>0:\n",
        "    color = (point_cloud_Camera[:, 3])\n",
        "    color_plot = np.array([COLOR_MAP[int(c)] for c in color])\n",
        "    # print(color_plot)\n",
        "    plt.scatter(point_cloud_Camera[:,0],point_cloud_Camera[:,2],s=0.1 , color = color_plot/255)\n",
        "  plt.xlabel('Z')\n",
        "  plt.ylabel('X')\n",
        "  plt.xlim(-25, 25)\n",
        "  plt.ylim(0, 25) \n",
        "\n",
        "  plt.savefig('saved_figure.jpg')\n",
        "  im = cv.imread('saved_figure.jpg')\n",
        "  frames.append(im)\n",
        "  plt.close()\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HY9F8P83103r"
      },
      "outputs": [],
      "source": [
        "# # Get Map of Time frames of SAR to Camera, by finding nearest frames\n",
        "\n",
        "# import pickle\n",
        "# # os.chdir(\"Capstone-Updated/SAR+Camera_Fusion\")\n",
        "\n",
        "# with open('camera_times.pickle', 'rb') as file:\n",
        "#     camera_times = pickle.load(file)\n",
        "\n",
        "# with open('sar_tracklog.pickle', 'rb') as file:\n",
        "#     SAR_tracklog = pickle.load(file)\n",
        "\n",
        "# SAR_tracklog = np.array(SAR_tracklog)\n",
        "# SAR_times = SAR_tracklog[:,0]\n",
        "\n",
        "# timestamp_map = np.zeros(len(SAR_times))\n",
        "\n",
        "# j=0\n",
        "# for i in range(len(SAR_times)):\n",
        "#   while camera_times[j]<SAR_times[i]:\n",
        "#     j+=1\n",
        "#   timestamp_map[i] = j\n",
        "# # print(timestamp_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PPgKg5mNzpBh",
        "outputId": "f6c727ce-bc7d-4c89-a92e-4f5b6a8d30f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ],
      "source": [
        "frames=[]\n",
        "video_radar = cv.VideoCapture(RADAR_VIDEO)\n",
        "# num_frames = 200\n",
        "\n",
        "\n",
        "A = np.zeros((num_frames,624,1250))\n",
        "\n",
        "SAR_points = []\n",
        "\n",
        "try:\n",
        "    for i in range(num_frames):\n",
        "        _, frame = video_radar.read()\n",
        "        if not _: break\n",
        "        A[i], points_SAR= GetThreshold_Binary(frame)\n",
        "        # A[i], points_SAR = GetThreshold_Adaptive(frame)\n",
        "        # A[i], points_SAR = GetThreshold_Gaussian(frame)\n",
        "\n",
        "        points_camera = np.array(pc3darray_SAR_frame[i])\n",
        "        Plot_Camera_with_SAR(points_SAR, points_camera)\n",
        "        SAR_points.append(points_SAR)\n",
        "\n",
        "        IPython.display.clear_output(wait=True)\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n",
        "\n",
        "\n",
        "height, width, layers = frames[0].shape\n",
        "size = (width,height)\n",
        "fourcc = cv.VideoWriter_fourcc(*'MJPG')\n",
        "out = cv.VideoWriter('SARandCamera_pointcloud.avi', fourcc, 30.0, size)\n",
        " \n",
        "for i in range(len(frames)):\n",
        "    out.write(frames[i])\n",
        "out.release()\n",
        "\n",
        "\n",
        "SAR_points = np.array(SAR_points)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bjBDj97y9k18",
        "3jKEwPDK-Edp",
        "Kja3q7PyhB4H",
        "s2Fd1wKnhUqf",
        "V5OVRWSXYUo4",
        "HKc6MK8Hy_YJ"
      ],
      "name": "SAR_and_Camera_PointCloud.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "369c511214ca49fdaa371b31f9b460ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e4260cf5e474aeba86cfcffeeaa434c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68c7b7f676f44342baecebdfd269fd76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e84fffe5024c4657807ddd9d3400ee28",
            "placeholder": "​",
            "style": "IPY_MODEL_f63a2c3e4f66442999a9634d79829438",
            "value": " 1.28G/1.28G [00:17&lt;00:00, 43.7MB/s]"
          }
        },
        "7b51a34012af49e18f6c82c2c9dc9d6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e79f101c1b7442385207bc9fe00ddc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6967a9a72f74b5682b3e3cdccdde067",
              "IPY_MODEL_c1104dc2c35e4457a7909ec8561a9aca",
              "IPY_MODEL_68c7b7f676f44342baecebdfd269fd76"
            ],
            "layout": "IPY_MODEL_4e4260cf5e474aeba86cfcffeeaa434c"
          }
        },
        "a6967a9a72f74b5682b3e3cdccdde067": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b51a34012af49e18f6c82c2c9dc9d6b",
            "placeholder": "​",
            "style": "IPY_MODEL_f9251aaa927f4cc996241ae1f75cd831",
            "value": "100%"
          }
        },
        "aab355a9fbb746688d3eae5a2eb1559d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1104dc2c35e4457a7909ec8561a9aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_369c511214ca49fdaa371b31f9b460ae",
            "max": 1376378527,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aab355a9fbb746688d3eae5a2eb1559d",
            "value": 1376378527
          }
        },
        "e84fffe5024c4657807ddd9d3400ee28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f63a2c3e4f66442999a9634d79829438": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9251aaa927f4cc996241ae1f75cd831": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}