{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mmengyw/Capstone-Updated/blob/main/Depth%20Prediction/DepthPrediction_to_PointCloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjBDj97y9k18"
      },
      "source": [
        "# Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vwz1UzMu9mF-",
        "outputId": "1acf0084-0d77-4049-9cc8-2e8d64ae2d5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "# Tensorflow\n",
        "import tensorflow.compat.v1 as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# I/O libraries\n",
        "import os\n",
        "from io import BytesIO\n",
        "import tarfile\n",
        "import tempfile\n",
        "from six.moves import urllib\n",
        "\n",
        "# Helper libraries\n",
        "import matplotlib\n",
        "import torch\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2 as cv\n",
        "from tqdm import tqdm\n",
        "import IPython\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Comment this out if you want to see Deprecation warnings\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "#test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hwkf2tnYQKAT",
        "outputId": "a9a9c9e2-775b-4379-f88a-98f2ad01bf40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ],
      "source": [
        "pip install timm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jKEwPDK-Edp"
      },
      "source": [
        "# Functions for Image Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24Xfj_GZ-FWL"
      },
      "outputs": [],
      "source": [
        "class DeepLabModel(object):\n",
        "    \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
        "\n",
        "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
        "\n",
        "    def __init__(self, tarball_path):\n",
        "        \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
        "        self.graph = tf.Graph()\n",
        "        graph_def = None\n",
        "\n",
        "        # Extract frozen graph from tar archive.\n",
        "        tar_file = tarfile.open(tarball_path)\n",
        "        for tar_info in tar_file.getmembers():\n",
        "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
        "                file_handle = tar_file.extractfile(tar_info)\n",
        "                graph_def = tf.GraphDef.FromString(file_handle.read())\n",
        "                break\n",
        "        tar_file.close()\n",
        "\n",
        "        if graph_def is None:\n",
        "            raise RuntimeError('Cannot find inference graph in tar archive.')\n",
        "\n",
        "        with self.graph.as_default():\n",
        "            tf.import_graph_def(graph_def, name='')\n",
        "        self.sess = tf.Session(graph=self.graph)\n",
        "\n",
        "    def run(self, image, INPUT_TENSOR_NAME = 'ImageTensor:0', OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'):\n",
        "        \"\"\"Runs inference on a single image.\n",
        "\n",
        "        Args:\n",
        "            image: A PIL.Image object, raw input image.\n",
        "            INPUT_TENSOR_NAME: The name of input tensor, default to ImageTensor.\n",
        "            OUTPUT_TENSOR_NAME: The name of output tensor, default to SemanticPredictions.\n",
        "\n",
        "        Returns:\n",
        "            resized_image: RGB image resized from original input image.\n",
        "            seg_map: Segmentation map of `resized_image`.\n",
        "        \"\"\"\n",
        "        width, height = image.size\n",
        "        target_size = (2049,1025)  # size of Cityscapes images\n",
        "        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
        "        batch_seg_map = self.sess.run(\n",
        "            OUTPUT_TENSOR_NAME,\n",
        "            feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
        "        seg_map = batch_seg_map[0]  # expected batch size = 1\n",
        "        if len(seg_map.shape) == 2:\n",
        "            seg_map = np.expand_dims(seg_map,-1)  # need an extra dimension for cv.resize\n",
        "        seg_map = cv.resize(seg_map, (width,height), interpolation=cv.INTER_NEAREST)\n",
        "        return seg_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCIUhvfV-MiI"
      },
      "outputs": [],
      "source": [
        "def create_label_colormap():\n",
        "    \"\"\"Creates a label colormap used in Cityscapes segmentation benchmark.\n",
        "\n",
        "    Returns:\n",
        "        A Colormap for visualizing segmentation results.\n",
        "    \"\"\"\n",
        "    colormap = np.array([\n",
        "        [128,  64, 128],\n",
        "        [244,  35, 232],\n",
        "        [ 70,  70,  70],\n",
        "        [102, 102, 156],\n",
        "        [190, 153, 153],\n",
        "        [153, 153, 153],\n",
        "        [250, 170,  30],\n",
        "        [220, 220,   0],\n",
        "        [107, 142,  35],\n",
        "        [152, 251, 152],\n",
        "        [ 70, 130, 180],\n",
        "        [220,  20,  60],\n",
        "        [255,   0,   0],\n",
        "        [  0,   0, 142],\n",
        "        [  0,   0,  70],\n",
        "        [  0,  60, 100],\n",
        "        [  0,  80, 100],\n",
        "        [  0,   0, 230],\n",
        "        [119,  11,  32],\n",
        "        [  0,   0,   0]], dtype=np.uint8)\n",
        "    return colormap\n",
        "\n",
        "\n",
        "def label_to_color_image(label):\n",
        "    \"\"\"Adds color defined by the dataset colormap to the label.\n",
        "\n",
        "    Args:\n",
        "        label: A 2D array with integer type, storing the segmentation label.\n",
        "\n",
        "    Returns:\n",
        "        result: A 2D array with floating type. The element of the array\n",
        "            is the color indexed by the corresponding element in the input label\n",
        "            to the PASCAL color map.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If label is not of rank 2 or its value is larger than color\n",
        "            map maximum entry.\n",
        "    \"\"\"\n",
        "    if label.ndim != 2:\n",
        "        raise ValueError('Expect 2-D input label')\n",
        "\n",
        "    colormap = create_label_colormap()\n",
        "\n",
        "    if np.max(label) >= len(colormap):\n",
        "        raise ValueError('label value too large.')\n",
        "\n",
        "    return colormap[label]\n",
        "\n",
        "\n",
        "def vis_segmentation(image, seg_map):\n",
        "    \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n",
        "\n",
        "    plt.subplot(grid_spec[0])\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title('input image')\n",
        "\n",
        "    plt.subplot(grid_spec[1])\n",
        "    seg_image = label_to_color_image(seg_map).astype(np.uint8)\n",
        "    plt.imshow(seg_image)\n",
        "    plt.axis('off')\n",
        "    plt.title('segmentation map')\n",
        "\n",
        "    plt.subplot(grid_spec[2])\n",
        "    plt.imshow(image)\n",
        "    plt.imshow(seg_image, alpha=0.7)\n",
        "    plt.axis('off')\n",
        "    plt.title('segmentation overlay')\n",
        "\n",
        "    unique_labels = np.unique(seg_map)\n",
        "    ax = plt.subplot(grid_spec[3])\n",
        "    plt.imshow(FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n",
        "    ax.yaxis.tick_right()\n",
        "    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n",
        "    plt.xticks([], [])\n",
        "    ax.tick_params(width=0.0)\n",
        "    plt.grid('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "LABEL_NAMES = np.asarray([\n",
        "    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "    'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck',\n",
        "    'bus', 'train', 'motorcycle', 'bicycle', 'void'])\n",
        "\n",
        "COLOR_MAP = np.array([\n",
        "    [128,  64, 128],\n",
        "    [244,  35, 232],\n",
        "    [ 70,  70,  70],\n",
        "    [102, 102, 156],\n",
        "    [190, 153, 153],\n",
        "    [153, 153, 153],\n",
        "    [250, 170,  30],\n",
        "    [220, 220,   0],\n",
        "    [107, 142,  35],\n",
        "    [152, 251, 152],\n",
        "    [ 70, 130, 180],\n",
        "    [220,  20,  60],\n",
        "    [255,   0,   0],\n",
        "    [  0,   0, 142],\n",
        "    [  0,   0,  70],\n",
        "    [  0,  60, 100],\n",
        "    [  0,  80, 100],\n",
        "    [  0,   0, 230],\n",
        "    [119,  11,  32],\n",
        "    [  0,   0,   0]], dtype=np.uint8)\n",
        "\n",
        "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
        "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxKiAGG_-QYw",
        "outputId": "30b2acc4-ee58-4e65-a018-f714aced7d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading model, this might take a while...\n",
            "download completed! loading DeepLab model...\n",
            "model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = 'mobilenetv2_coco_cityscapes_trainfine'\n",
        "#MODEL_NAME = 'xception65_cityscapes_trainfine'\n",
        "\n",
        "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
        "_MODEL_URLS = {\n",
        "    'mobilenetv2_coco_cityscapes_trainfine':\n",
        "        'deeplabv3_mnv2_cityscapes_train_2018_02_05.tar.gz',\n",
        "    'xception65_cityscapes_trainfine':\n",
        "        'deeplabv3_cityscapes_train_2018_02_06.tar.gz',\n",
        "}\n",
        "_TARBALL_NAME = 'deeplab_model.tar.gz'\n",
        "\n",
        "model_dir = tempfile.mkdtemp()\n",
        "tf.gfile.MakeDirs(model_dir)\n",
        "\n",
        "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
        "print('downloading model, this might take a while...')\n",
        "urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME], download_path)\n",
        "print('download completed! loading DeepLab model...')\n",
        "\n",
        "MODEL = DeepLabModel(download_path)\n",
        "print('model loaded successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVnUbIVY96HU"
      },
      "source": [
        "# Initialize Midas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "46487160a20741b697054bc729593020",
            "1d76a21a421047928d49acb5b56212aa",
            "075e0b881c4b43cda5bff5ac853ee364",
            "87883b367c0948ad85f619ed5edae45c",
            "f1373e8a492f40acbc867bb4d3750ddf",
            "18c35b5b46434dc19372ec830bd872b8",
            "bf3494613f0843478014ae5a2dd61246",
            "64a8957c68814937bd86aa4dc47a57c5",
            "1eda6c5809434563a86f140b564e5556",
            "2432db9482424b408fc95dc1c38b4c8f",
            "9222dd8a6d1b4cc29e52453a926aa2a4"
          ]
        },
        "id": "uW1E_emiQatf",
        "outputId": "da51c081-c4bd-4bc7-a712-4f7ee2fb604b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/intel-isl/MiDaS/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "Downloading: \"https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt\" to /root/.cache/torch/hub/checkpoints/dpt_large-midas-2f21e586.pt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46487160a20741b697054bc729593020",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/1.28G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n",
        "#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n",
        "#model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n",
        "\n",
        "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywJyJpTzQtDR",
        "outputId": "29bc5fad-a825-4b5e-8d5a-45fb14c7af12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DPTDepthModel(\n",
              "  (pretrained): Module(\n",
              "    (model): VisionTransformer(\n",
              "      (patch_embed): PatchEmbed(\n",
              "        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
              "        (norm): Identity()\n",
              "      )\n",
              "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "      (blocks): Sequential(\n",
              "        (0): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (12): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (13): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (14): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (15): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (16): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (17): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (18): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (19): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (20): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (21): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (22): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (23): Block(\n",
              "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): Attention(\n",
              "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (drop_path): Identity()\n",
              "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (act): GELU()\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
              "      (pre_logits): Identity()\n",
              "      (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
              "    )\n",
              "    (act_postprocess1): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU()\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(4, 4))\n",
              "    )\n",
              "    (act_postprocess2): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU()\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (act_postprocess3): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU()\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (act_postprocess4): Sequential(\n",
              "      (0): ProjectReadout(\n",
              "        (project): Sequential(\n",
              "          (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "          (1): GELU()\n",
              "        )\n",
              "      )\n",
              "      (1): Transpose()\n",
              "      (2): Unflatten(dim=2, unflattened_size=torch.Size([24, 24]))\n",
              "      (3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (scratch): Module(\n",
              "    (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer4_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (refinenet1): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet2): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet3): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (refinenet4): FeatureFusionBlock_custom(\n",
              "      (out_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (resConfUnit1): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (resConfUnit2): ResidualConvUnit_custom(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): Identity()\n",
              "        )\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "    )\n",
              "    (output_conv): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): Interpolate()\n",
              "      (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (5): ReLU(inplace=True)\n",
              "      (6): Identity()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "midas.to(device)\n",
        "midas.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4Da96O0Q06Z",
        "outputId": "a787f996-9dc1-42a8-c232-4902d27e6889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        }
      ],
      "source": [
        "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "\n",
        "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
        "    transform = midas_transforms.dpt_transform\n",
        "else:\n",
        "    transform = midas_transforms.small_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN0kU6NJ1Ye5",
        "outputId": "488f5831-02c1-4792-8c57-908d58f2cf0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ],
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdsigyfAhITX",
        "outputId": "c9002bce-bb7d-4822-a319-b9db645f3381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Capstone-Updated'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (84/84), done.\u001b[K\n",
            "remote: Total 109 (delta 45), reused 14 (delta 5), pack-reused 18\u001b[K\n",
            "Receiving objects: 100% (109/109), 105.21 MiB | 36.83 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n",
            "Capstone-Updated  sample_data\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Mmengyw/Capstone-Updated.git\n",
        "!ls\n",
        "os.chdir(\"Capstone-Updated/Videos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kja3q7PyhB4H"
      },
      "source": [
        "# Conversion to meters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoPzjnl5hCCS"
      },
      "outputs": [],
      "source": [
        "nb_photo=34\n",
        "\n",
        "equiv=[[0.001,51],    #for extrapolation\n",
        "      [0.1,45],     #premier plan\n",
        "      [0.9,42.3],\n",
        "      [1.8,37.4],\n",
        "      [2.7,28.7],\n",
        "      [3.6,24.443365],\n",
        "      [4.5,22.058018],\n",
        "      [5.4,15.317413],\n",
        "      [6.3,14.677493],\n",
        "      [7.2,10.969739],\n",
        "      [8.1,10.883035],\n",
        "      [9,9.883035],\n",
        "      [9.9,8.058806],\n",
        "      [10.8,7.5158963],\n",
        "      [11.7,7.098169],\n",
        "      [12.6,6.111024],\n",
        "      [13.5,5.6323136],\n",
        "      [14.4,5.2216917],\n",
        "      [15.3,5],\n",
        "      [16.2,4.9529667],\n",
        "      [17.1,4.8],\n",
        "      [18,4.7],\n",
        "      [18.9,4.6],\n",
        "      [19.8,4.5],\n",
        "      [20.7,4.4],\n",
        "      [21.6,4.3],\n",
        "      [22.5,4.2],\n",
        "      [23.4,4.1],\n",
        "      [24.3,4],\n",
        "      [25.2,3.9],\n",
        "      [26.1,3.8],\n",
        "      [27,3.7],\n",
        "      [27.9,3.6],\n",
        "      [28.8,3.5],\n",
        "      [29.7,3.2],\n",
        "      [30.6,3],\n",
        "      [60,0.0],\n",
        "       \n",
        "      [120,-6],    #horizon\n",
        "       \n",
        "      [40,1.98],\n",
        "      [50,1.33]\n",
        "       \n",
        "       ]   #for extrapolation\n",
        "\n",
        "#=========================================================================================\n",
        "\n",
        "equiv2=[[1,41.05157], #1yard  0302\n",
        "        [1,42.18351],\n",
        "        [1,31.304607],\n",
        "        [1,25.090006],\n",
        "        [1,23.275448], #5yard 0306\n",
        "        [1,19.171278],\n",
        "        [1,17.472866],\n",
        "        [1,16.775742],\n",
        "        [1,15.820402],\n",
        "        [1,15.538459], #10yard  0311\n",
        "        [1,14.466544],\n",
        "        [1,12.707126],\n",
        "        [1,10.957558],\n",
        "      \n",
        "\n",
        "        [1,6.023936],#'''inacurrate'''\n",
        "        [1,9.797453],  #15yard 0316\n",
        "        [1,7.2150397],\n",
        "        [1,6.3944836],\n",
        "        [1,8.514687],\n",
        "        [1,7.735209],\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt4BAs1zhPp3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import Rbf, InterpolatedUnivariateSpline\n",
        "equiv=np.asarray(equiv)\n",
        "X = equiv[:,1]    #midas output\n",
        "Y=equiv[:,0]     #meters\n",
        "new_length = 25\n",
        "new_x = np.linspace(X.min(), X.max(), new_length)\n",
        "conv=sp.interpolate.interp1d(X, Y, kind='linear')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2Fd1wKnhUqf"
      },
      "source": [
        "#Increased contrast\n",
        "Just to get increased contrast, not values in meters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpDq3tgxhQV4"
      },
      "outputs": [],
      "source": [
        "coef_expand=[[-5,-1],\n",
        "        [1,10],\n",
        "        [5,25],\n",
        "        [10,35],\n",
        "        [15,42],\n",
        "        [35,43],\n",
        "        [45,50],\n",
        "        [51,51]\n",
        "        ]\n",
        "coef_expand=np.asarray(coef_expand)\n",
        "X = coef_expand[:,0]    #midas output\n",
        "Y=coef_expand[:,1]      #meters\n",
        "expand=sp.interpolate.interp1d(X, Y, kind='cubic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VDzNl3ehXh9"
      },
      "source": [
        "# Depth Prediction function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "oXTrq3DgKLSp",
        "outputId": "f5e735d7-4440-45e9-a696-7703c521411a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stream stopped.\n"
          ]
        }
      ],
      "source": [
        "!ls\n",
        "def depth_prediction_stream(image, frame, index):\n",
        "    \"\"\"Visualizes segmentation overlay view and stream it with IPython display.\"\"\"\n",
        "    img = frame\n",
        "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "\n",
        "    input_batch = transform(img).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      prediction = midas(input_batch)\n",
        "\n",
        "      prediction = torch.nn.functional.interpolate(\n",
        "        prediction.unsqueeze(1),\n",
        "        size=img.shape[:2],\n",
        "        mode=\"bicubic\",\n",
        "        align_corners=False,\n",
        "      ).squeeze()\n",
        "\n",
        "    output = prediction.cpu().numpy()\n",
        "    # output=expand(50*output/np.max(output)) # for better visualization\n",
        "    output=conv(50*output/np.max(output))  # to get values in meters\n",
        "    plt.imshow(output, cmap='plasma')\n",
        "\n",
        "    # # Show visualization in a streaming fashion.\n",
        "    f = BytesIO()\n",
        "    plt.savefig(f, format='jpeg')\n",
        "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
        "    f.close()\n",
        "    plt.close()\n",
        "\n",
        "def depth_prediction_video(frame, index):\n",
        "    \"\"\"Inferences DeepLab model on a video file and stream the visualization.\"\"\"\n",
        "    original_im = Image.fromarray(frame[..., ::-1])\n",
        "    depth_prediction_stream(original_im, frame, index)\n",
        "\n",
        "\n",
        "SAMPLE_VIDEO = 'test12.mp4'\n",
        "print('running deeplab on the sample video...')\n",
        "\n",
        "video = cv.VideoCapture(SAMPLE_VIDEO)\n",
        "# num_frames = 598  # uncomment to use the full sample video\n",
        "num_frames = 60\n",
        "\n",
        "try:\n",
        "    for i in range(num_frames):\n",
        "        _, frame = video.read()\n",
        "        if not _: break\n",
        "        depth_prediction_video(frame, i)\n",
        "        IPython.display.clear_output(wait=True)\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvPWay9cQOQw"
      },
      "outputs": [],
      "source": [
        "frames = []\n",
        "def depth_prediction_stream_2(image, frame, index):\n",
        "    \"\"\"Visualizes segmentation overlay view and stream it with IPython display.\"\"\"\n",
        "    img = frame\n",
        "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "\n",
        "    input_batch = transform(img).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      prediction = midas(input_batch)\n",
        "\n",
        "      prediction = torch.nn.functional.interpolate(\n",
        "        prediction.unsqueeze(1),\n",
        "        size=img.shape[:2],\n",
        "        mode=\"bicubic\",\n",
        "        align_corners=False,\n",
        "      ).squeeze()\n",
        "\n",
        "    output = prediction.cpu().numpy()\n",
        "    output=expand(50*output/np.max(output)) # for better visualization\n",
        "    # output=conv(50*output/np.max(output))  # to get values in meters\n",
        "    plt.imshow(output,cmap='plasma')\n",
        "\n",
        "    plt.savefig('saved_figure.jpg')\n",
        "    im = cv.imread('saved_figure.jpg')\n",
        "    frames.append(im)\n",
        "    plt.close()\n",
        "\n",
        "def depth_prediction_video_2(frame, index):\n",
        "    \"\"\"Inferences DeepLab model on a video file and stream the visualization.\"\"\"\n",
        "    original_im = Image.fromarray(frame[..., ::-1])\n",
        "    depth_prediction_stream_2(original_im, frame, index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "QSCltJbRO62j",
        "outputId": "8855af95-35ad-47df-c502-09a4be15c574"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stream stopped.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7c65b235ab30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mfourcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoWriter_fourcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m'MJPG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "video = cv.VideoCapture(SAMPLE_VIDEO)\n",
        "num_frames = 50  # uncomment to use the full sample video\n",
        "try:\n",
        "    for i in range(num_frames):\n",
        "        _, frame = video.read()\n",
        "        if not _: break\n",
        "        depth_prediction_video_2(frame, i)\n",
        "        IPython.display.clear_output(wait=True)\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n",
        "\n",
        "\n",
        "height, width, layers = frames[0].shape\n",
        "size = (width,height)\n",
        "fourcc = cv.VideoWriter_fourcc(*'MJPG')\n",
        "out = cv.VideoWriter('DepthPrediction.avi', fourcc, 20.0, size)\n",
        " \n",
        "for i in range(len(frames)):\n",
        "    out.write(frames[i])\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyvRLfn6-Yaz"
      },
      "outputs": [],
      "source": [
        "img = frame\n",
        "img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "\n",
        "input_batch = transform(img).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  prediction = midas(input_batch)\n",
        "\n",
        "  prediction = torch.nn.functional.interpolate(\n",
        "    prediction.unsqueeze(1),\n",
        "    size=img.shape[:2],\n",
        "    mode=\"bicubic\",\n",
        "    align_corners=False,\n",
        "  ).squeeze()\n",
        "\n",
        "output = prediction.cpu().numpy()\n",
        "# output=expand(50*output/np.max(output)) # for better visualization\n",
        "output=conv(50*output/np.max(output))  # to get values in meters\n",
        "fig = plt.figure(figsize=(16,12))\n",
        "plt.imshow(output,cmap='plasma')\n",
        "\n",
        "# plt.imshow(frame, cmap='plasma')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_AAWGyuFi-3"
      },
      "outputs": [],
      "source": [
        "# print(output[0:400])\n",
        "plt.imshow(output[0:400,:],cmap='plasma')\n",
        "print(np.max(output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJfWnzX86o3Z"
      },
      "outputs": [],
      "source": [
        "flat_out = output.flatten()\n",
        "print(np.max(flat_out))\n",
        "plt.hist(flat_out, bins = 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie-sWMxW_laU"
      },
      "source": [
        "# Create Point Cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHsHZlRF0ORx"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def depth2pcd(depth):\n",
        "    width = depth.shape[1]\n",
        "    height = depth.shape[0]\n",
        "    fx= 926.9796142578125\n",
        "    fy= 924.431884765625\n",
        "    cx= 790.234375\n",
        "    cy= 617.5499267578125\n",
        "    points = []\n",
        "    # fig = plt.figure()\n",
        "\n",
        "    for v in range(0, width, 10):\n",
        "        for u in range(0, height, 10):\n",
        "            Z = depth[u][v]\n",
        "            if Z == 0:\n",
        "                continue\n",
        "            X_cam = (v - cx)\n",
        "            \n",
        "            Y_cam = -(u - cy)\n",
        "            \n",
        "            X = X_cam/fx*Z\n",
        "            Y = Y_cam/fy*Z\n",
        "            # if Y>0 and Y<5:\n",
        "            points.append([X, Y, Z])\n",
        "            \n",
        "    return np.array(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLO6fn3d89pw"
      },
      "outputs": [],
      "source": [
        "pc_3d = depth2pcd(output)\n",
        "# print(pc_3d)\n",
        "\n",
        "\n",
        "x = pc_3d[:, 0]\n",
        "y = pc_3d[:, 1]\n",
        "z = pc_3d[:, 2]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import proj3d\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.view_init(270,0)\n",
        "ax.scatter(x,y,z)\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "# plt.zlabel('Z')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6vP63N32i_0"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10,16))\n",
        "plt.scatter(z,y)\n",
        "plt.xlabel('Z')\n",
        "plt.ylabel('X')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpSifr2Hrbqz"
      },
      "outputs": [],
      "source": [
        "x_trunc = []\n",
        "y_trunc = []\n",
        "z_trunc = []\n",
        "\n",
        "for i in range(len(x)):\n",
        "  if y[i]>-1:\n",
        "    x_trunc.append(x[i])\n",
        "    y_trunc.append(y[i])\n",
        "    z_trunc.append(z[i])\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "plt.scatter(x_trunc,z_trunc)\n",
        "plt.xlabel('Z')\n",
        "plt.ylabel('X')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8al3wvJsoWw"
      },
      "outputs": [],
      "source": [
        "frames = []\n",
        "def depth_prediction_stream_3D(image, frame, index):\n",
        "    \"\"\"Visualizes segmentation overlay view and stream it with IPython display.\"\"\"\n",
        "    img = frame\n",
        "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "\n",
        "    input_batch = transform(img).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      prediction = midas(input_batch)\n",
        "\n",
        "      prediction = torch.nn.functional.interpolate(\n",
        "        prediction.unsqueeze(1),\n",
        "        size=img.shape[:2],\n",
        "        mode=\"bicubic\",\n",
        "        align_corners=False,\n",
        "      ).squeeze()\n",
        "\n",
        "    output = prediction.cpu().numpy()\n",
        "    # output=expand(50*output/np.max(output)) # for better visualization\n",
        "    output=conv(50*output/np.max(output))  # to get values in meters\n",
        "\n",
        "    # plt.imshow(output, cmap='plasma')\n",
        "\n",
        "    pc_3d = depth2pcd(output)\n",
        "    x = pc_3d[:, 0]\n",
        "    y = pc_3d[:, 1]\n",
        "    z = pc_3d[:, 2]\n",
        "    fig = plt.figure(figsize=(12,6))\n",
        "    plt.scatter(x,z)\n",
        "    plt.xlabel('Z')\n",
        "    plt.ylabel('X')\n",
        "    plt.xlim(-60, 60)\n",
        "    plt.ylim(0, 60) \n",
        "\n",
        "    plt.savefig('saved_figure.jpg')\n",
        "    im = cv.imread('saved_figure.jpg')\n",
        "    frames.append(im)\n",
        "    plt.close()\n",
        "\n",
        "def depth_prediction_video_3D(frame, index):\n",
        "    \"\"\"Inferences DeepLab model on a video file and stream the visualization.\"\"\"\n",
        "    original_im = Image.fromarray(frame[..., ::-1])\n",
        "    depth_prediction_stream_3D(original_im, frame, index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRusidZBsmyw"
      },
      "outputs": [],
      "source": [
        "frames=[]\n",
        "video = cv.VideoCapture(SAMPLE_VIDEO)\n",
        "# num_frames = 598  # uncomment to use the full sample video\n",
        "num_frames = 50 \n",
        "try:\n",
        "    for i in range(num_frames):\n",
        "        _, frame = video.read()\n",
        "        if not _: break\n",
        "        depth_prediction_video_3D(frame, i)\n",
        "        IPython.display.clear_output(wait=True)\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n",
        "\n",
        "\n",
        "height, width, layers = frames[0].shape\n",
        "size = (width,height)\n",
        "fourcc = cv.VideoWriter_fourcc(*'MJPG')\n",
        "out = cv.VideoWriter('3D_pointcloud.avi', fourcc, 20.0, size)\n",
        " \n",
        "for i in range(len(frames)):\n",
        "    out.write(frames[i])\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWvqyy5Hy715"
      },
      "outputs": [],
      "source": [
        "plt.imshow(frames[9])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q99un1SR_xll"
      },
      "source": [
        "# Image Segmentation + Depth Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-QtEEGNGcuH"
      },
      "outputs": [],
      "source": [
        "def depth2pcd_segm(depth,seg_map):\n",
        "    width = depth.shape[1]\n",
        "    height = depth.shape[0]\n",
        "    fx= 926.9796142578125\n",
        "    fy= 924.431884765625\n",
        "    cx= 790.234375\n",
        "    cy= 617.5499267578125\n",
        "    points = []\n",
        "    # fig = plt.figure()\n",
        "    objects_needed = {'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n",
        "    'traffic sign', 'vegetation', 'terrain', 'person', 'rider', 'car', 'truck',\n",
        "    'bus', 'train', 'motorcycle', 'bicycle'}\n",
        "    # objects_car = {  'person', 'rider', 'car', 'truck',\n",
        "    # 'bus', 'train', 'motorcycle', 'bicycle'}\n",
        "    objects_car = {'road'}\n",
        "\n",
        "\n",
        "    for v in range(0, width, 10):\n",
        "        for u in range(0, height, 10):\n",
        "            Z = depth[u][v]\n",
        "            color = seg_map[u][v]\n",
        "            if Z == 0:\n",
        "                continue\n",
        "            X_cam = (v - cx)\n",
        "            \n",
        "            Y_cam = -(u - cy)\n",
        "            \n",
        "            X = X_cam/fx*Z\n",
        "            Y = Y_cam/fy*Z\n",
        "            if LABEL_NAMES[color] in objects_car:\n",
        "              points.append([X, Y, Z,color])\n",
        "            \n",
        "    return np.array(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "refmmrWb_2eH"
      },
      "outputs": [],
      "source": [
        "def prediction_stream(image, seg_map, seg_data, frame, index):\n",
        "    \"\"\"Visualizes segmentation overlay view and stream it with IPython display.\"\"\"\n",
        "    for i in range(len(seg_map)):\n",
        "        for j in range(len(seg_map[i])):\n",
        "                seg_data[i][j] = LABEL_NAMES[seg_map[i][j]]\n",
        "                \n",
        "\n",
        "    \n",
        "    img = frame\n",
        "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
        "\n",
        "    input_batch = transform(img).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      prediction = midas(input_batch)\n",
        "\n",
        "      prediction = torch.nn.functional.interpolate(\n",
        "        prediction.unsqueeze(1),\n",
        "        size=img.shape[:2],\n",
        "        mode=\"bicubic\",\n",
        "        align_corners=False,\n",
        "      ).squeeze()\n",
        "\n",
        "    output = prediction.cpu().numpy()\n",
        "    # output=expand(50*output/np.max(output)) # for better visualization\n",
        "    output=conv(50*output/np.max(output))  # to get values in meters\n",
        "\n",
        "    # plt.imshow(output, cmap='plasma')\n",
        "\n",
        "    pc_3d = depth2pcd_segm(output,seg_map)\n",
        "    x = pc_3d[:, 0]\n",
        "    y = pc_3d[:, 1]\n",
        "    z = pc_3d[:, 2]\n",
        "    color = pc_3d[:, 3]\n",
        "    color_plot = np.zeros((len(color),3))\n",
        "    # color_plot = []\n",
        "    for i in range(len(color)):\n",
        "      # print(color[i])\n",
        "      # print(COLOR_MAP[int(color[i])])\n",
        "      color_plot[i] = COLOR_MAP[int(color[i])]\n",
        "      # color_plot.append(COLOR_MAP[color[i]])\n",
        "    # color_plot = np.array(color_plot)\n",
        "    fig = plt.figure(figsize=(12,6))\n",
        "    plt.scatter(x,z,c=color_plot/255)\n",
        "    plt.xlabel('Z')\n",
        "    plt.ylabel('X')\n",
        "    plt.xlim(-60, 60)\n",
        "    plt.ylim(0, 60) \n",
        "\n",
        "    plt.savefig('saved_figure.jpg')\n",
        "    im = cv.imread('saved_figure.jpg')\n",
        "    frames.append(im)\n",
        "    plt.close()\n",
        "    # plt.imshow(expand(output), cmap='plasma')\n",
        "    # A = np.stack([seg_data,output])\n",
        "    A = np.stack([seg_map,output])\n",
        "\n",
        "    # # Show visualization in a streaming fashion.\n",
        "    # f = BytesIO()\n",
        "    # plt.savefig(f, format='jpeg')\n",
        "    # IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
        "    # f.close()\n",
        "    # plt.close()\n",
        "    return A\n",
        "def prediction_video(frame, index):\n",
        "    \"\"\"Inferences DeepLab model on a video file and stream the visualization.\"\"\"\n",
        "    original_im = Image.fromarray(frame[..., ::-1])\n",
        "    seg_map = MODEL.run(original_im)\n",
        "    seg_data = np.full((len(seg_map),len(seg_map[0])),'nullvoidnada')\n",
        "    filled_seg_data = prediction_stream(original_im, seg_map, seg_data, frame, index)\n",
        "    # print(filled_seg_data)\n",
        "    return filled_seg_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7a_YXpaEno-",
        "outputId": "56eab7fb-93e3-4e79-e86d-8fb0ac791923"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stream stopped.\n"
          ]
        }
      ],
      "source": [
        "frames = []\n",
        "\n",
        "SAMPLE_VIDEO = 'test12.mp4'\n",
        "print('running deeplab on the sample video...')\n",
        "\n",
        "video = cv.VideoCapture(SAMPLE_VIDEO)\n",
        "# num_frames = 598  # uncomment to use the full sample video\n",
        "num_frames = 100\n",
        "\n",
        "try:\n",
        "    for i in range(num_frames):\n",
        "        _, frame = video.read()\n",
        "        if not _: break\n",
        "        filled_seg_DATA = prediction_video(frame, i)\n",
        "        IPython.display.clear_output(wait=True)\n",
        "except KeyboardInterrupt:\n",
        "    plt.close()\n",
        "    print(\"Stream stopped.\")\n",
        "# print(filled_seg_DATA)\n",
        "\n",
        "\n",
        "height, width, layers = frames[0].shape\n",
        "size = (width,height)\n",
        "fourcc = cv.VideoWriter_fourcc(*'MJPG')\n",
        "out = cv.VideoWriter('3D_pointcloud_road.avi', fourcc, 20.0, size)\n",
        " \n",
        "for i in range(len(frames)):\n",
        "    out.write(frames[i])\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WYBqJaKHT8k",
        "outputId": "a9ddcbe1-813f-4259-f470-95d5a61bdd26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.  2.  2. ...  8.  8.  8.]\n",
            " [ 2.  2.  2. ...  8.  8.  8.]\n",
            " [ 2.  2.  2. ...  8.  8.  8.]\n",
            " ...\n",
            " [13. 13. 13. ... 13. 13. 13.]\n",
            " [13. 13. 13. ... 13. 13. 13.]\n",
            " [13. 13. 13. ... 13. 13. 13.]]\n"
          ]
        }
      ],
      "source": [
        "print(filled_seg_DATA[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgjClbE21Fmk",
        "outputId": "6a3f92db-542e-4683-cbd9-9e59ea24b4ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2], [3, 4], [5, 6]]\n"
          ]
        }
      ],
      "source": [
        "a = [1,2]\n",
        "b = [3,4]\n",
        "c = [5,6]\n",
        "a = [a,b]\n",
        "a.append(c)\n",
        "print(a)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bjBDj97y9k18",
        "Kja3q7PyhB4H",
        "s2Fd1wKnhUqf",
        "ie-sWMxW_laU"
      ],
      "name": "DepthPrediction_to_PointCloud.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46487160a20741b697054bc729593020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1d76a21a421047928d49acb5b56212aa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_075e0b881c4b43cda5bff5ac853ee364",
              "IPY_MODEL_87883b367c0948ad85f619ed5edae45c",
              "IPY_MODEL_f1373e8a492f40acbc867bb4d3750ddf"
            ]
          }
        },
        "1d76a21a421047928d49acb5b56212aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "075e0b881c4b43cda5bff5ac853ee364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_18c35b5b46434dc19372ec830bd872b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bf3494613f0843478014ae5a2dd61246"
          }
        },
        "87883b367c0948ad85f619ed5edae45c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_64a8957c68814937bd86aa4dc47a57c5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1376378527,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1376378527,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1eda6c5809434563a86f140b564e5556"
          }
        },
        "f1373e8a492f40acbc867bb4d3750ddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2432db9482424b408fc95dc1c38b4c8f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.28G/1.28G [00:16&lt;00:00, 109MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9222dd8a6d1b4cc29e52453a926aa2a4"
          }
        },
        "18c35b5b46434dc19372ec830bd872b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bf3494613f0843478014ae5a2dd61246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "64a8957c68814937bd86aa4dc47a57c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1eda6c5809434563a86f140b564e5556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2432db9482424b408fc95dc1c38b4c8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9222dd8a6d1b4cc29e52453a926aa2a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
